{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parte 1\n",
      "a)\n",
      "La técnica de análisis conjunto es usada en el proceso de desarrollo de productos nuevos, tanto para diseño, precios y “targeting”. Enfocándose en el proceso de diseño, hay varios pasos a seguir.\n",
      "Definir el producto: esto se hace definiendo el producto como una colección de atributos. Esto sirve ya que podemos inferir la importancia del atributo y que tan deseado es.\n",
      "Estudio de diseño: este consiste en 5 etapas. \n",
      "Determinar atributos relevantes: si un atributo no es relevante se va a descartar, pero si falta un atributo relevante no hay cómo saberlo.\n",
      "Representación de estímulos: esta parte consiste en ver como presentar los productos al destinatario. Se puede hacer principalmente de dos formas:\n",
      "Metodo perfil completo: cada producto es descrito con todos sus atributos relevantes. Es la versión más cercana a la toma de decisiones reales del consumidor.\n",
      "Perfil parcial: este solo entrega un subconjunto de la lista de atributos. Con este método se entiende mejor el nivel deseado e importancia de atributos secundarios.\n",
      "Tipo de respuesta: consiste en que los encuestados entreguen juicios del producto como ranking o con ratings. En métodos de rankings hay que considerar explícitamente las opciones relevantes. En métodos de comparación el encuestado se le entrega un par de productos y debe elegir entre uno o el otro.\n",
      "Criterio: está relacionado con el tipo de respuesta, los dos criterios principales son preferencia e intención de compra.\n",
      "Métodos de análisis de datos: depende al tipo de datos recolectados, si se usan rankings aplicamos MONANOVA, si se usa ratings Regresión Simple y si vemos probabilidad de compra usamos el modelo Logit. Es importante notar que si se usan rankings no podemos saber por cuanto una alternativa es preferida por sobre la otra.\n",
      "Analizando los Outputs: no se puede asumir que los consumidores tienen el mismo sistema de valores, los tres tipos de análisis mayores son:\n",
      "Análisis agregado: consiste en promediar las utilidades de cada nivel de atributos con el propósito de entregar al analista una medida de por qué atributos son más importantes.\n",
      "Análisis segmentado: consiste en diferenciar entre los clientes, es decir hacer un análisis agregado pero segmentando por grupos. Esto puede cambiar mucho los resultados y es crucial para la selección de mercados.\n",
      "Simulación de escenarios: consiste en predecir cantidad de ventas en varios escenarios.\n",
      "Exactitud de métodos conjuntos: hay pocos estudios que verifiquen la eficacia de este método. Pero hay distintos métodos utilizados para testear eficacia. El test extremo es si los resultados predichos ocurren o no.\n",
      "\n",
      "Un ejemplo en el que puede ser aplicado esta técnica, es en el diseño de un nuevo vehículo, este se le pueden definir atributos relevantes como diseño no muy vistoso, cómodo, económico, etc… Después al hacer el estudio de diseño, hacemos un estudio de perfil parcial con lo cual podemos rankear cada atributo pidiéndole al encuestado que hagan un ranking de los atributos, aplicamos MONANOVA para analizar los datos, finalmente procedemos a analizar los outputs para saber la importancia de los atributos nombrados y diseñamos el producto en base a eso.\n",
      "\n",
      "b)\n",
      "SERVQUAL es un framework de manejo de calidad, se usan 11 pasos para desarrollar la escala Servicio/Calidad.\n",
      "Paso 1: consiste en definir la calidad de servicio como la discrepancia entre las percepciones de los consumidores de servicios ofrecidos por una firma particular y la expectativa sobre otras firmas ofreciendo el servicio.\n",
      "Paso 2: Identificar 10 dimensiones que contienen el constructo servicio-calidad. Estas son tangibles, confiables, sensibilidad, comunicación, credibilidad, seguridad, competencia, cortesía, entendimiento, el cliente y acceso.\n",
      "Paso 3: Generar 97 items representado las 10 dimensiones. Cada ítem se separa en dos enunciados, uno para medir las expectativas de las firmas en general en la categoría de servicios siendo investigada y el otro es medir percepciones sobre la firma particular en la que el servicio de calidad está siendo medido.\n",
      "Paso 4: Recopilar los datos de las expectativas y percepciones de 200 encuestados, fijándose que cada uno haya usado uno de los servicios recientemente.\n",
      "Paso 5: Filtrar la escala usando la siguiente secuencia iterativa:\n",
      "Computar el coeficiente alfa y correlaciones ítem a total para cada dimensión.\n",
      "Eliminar ítems en que la correlación ítem a total sea baja en que su eliminación aumente el coeficiente alfa.\n",
      "Hacer un análisis factor para verificar la dimensionalidad de la escala promedio.\n",
      "Reasignar los ítems y reestructurar las dimensiones donde sea necesario.\n",
      "Paso 6: al terminar la iteración, identificar 34 ítems los cuales representan las 7 dimensiones.\n",
      "Paso 7: Recopilar los datos de expectativas y percepciones de 4 muestras independientes de 200 personas (estas deben seguir las mismas exigencias que en el paso 4).\n",
      "Paso 8: Evaluar y seguir filtrando los 34 ítems usando la misma secuencia iterativa del paso 5 en cada uno de las 4 muestras\n",
      "Paso 9: Identificar 22 ítems, los cuales representan 5 dimensiones.\n",
      "Paso 10: Evaluar la confiabilidad del SERQUAL y re-analizar los datos originales tomados en el paso 4 perteneciente a 22 ítems para verificar la consistencia interna de la escala y dimensionalidad.\n",
      "Paso 11: Hacer una evaluación de la validez del servicio. Para esto se evalúan dos aspectos. Primero que tan rigurosamente fue categorizado el constructo y el dominio como fue explicado. Después la extensión a la cual la escala de ítems representa el dominio del constructo.\n",
      "\n",
      "El método SERVQUAL es útil para una aerolínea, ya que esta trabaja en varios aeropuertos y si quiere hacer una comparación en la calidad del servicio en distintas ubicaciones, debe aplicar el proceso en cada uno y lograr tener una idea de la percepción del cliente y su expectativa en distintos lados. Esto sería útil porque la aerolínea sabría qué medidas especializadas debería tomar en distintas ubicaciones para lograr la satisfacción máxima del cliente.\n",
      "\n",
      "c) \n",
      "\n",
      "\n",
      "d)\n",
      "SERVQUAL es válido cuando su contenido es válido a nivel calificativo, es decir que el rigor en que fue construido y su dominio es explicado y también la extensión en la cual los ítems en la escala representan el dominio del constructo es elevada. No es válida cuando se intenta comparar servicios de distintas áreas, ya que carece de rigor el constructo que se debe realizar. Las ventajas del sistema son que entrega un buen punto de comparación entre áreas similares ya que considera que se compite contra la expectativa de la competencia más que su valor real. Las desventajas son que está basado en muchos supuestos y no hay garantías de que el cliente realmente percibe el sistema como percepción vs expectativa.\n",
      "\n",
      "\n",
      "Parte 2\n",
      "a)\n",
      "MODELOS:\n",
      "\n",
      "DRIFT: Es una variación del método de naive, que permite que las predicciones incrementen o decrezcan en el tiempo, con una magnitud igual al promedio del cambio entre el tiempo T elegido de la muestra y el tiempo inicial. Es decir, tomar una línea entre el primer evento y el evento T y proyectarlo a futuro.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MODELO DE INDICE ESTACIONAL MULTIPLICATIVO:\n",
      "\t\n",
      "\tTiene por objetivo medir el impacto de la inflación estacional en la variable de estudio mediante la utilización de índice multiplicativo en la vecindad de 1.\n",
      "\n",
      "\tEl modelo se descompone de la siguiente forma, utilizando el índice estacional de la media móvil centrada, Y es la demanda o ventas, S en la estacionalidad, T es la tendencia y E es el error. Ademas, t es el tiempo. En este caso la estacionalidad es proporcional a la tendencia\n",
      "\n",
      "Y_t = S_t ∗ T_t ∗ E_t\n",
      "\n",
      "\tSi la distribución de la tendencia presenta una forma lineal entonces se realiza la siguiente adaptación donde Tt = P ∗ (1 + i) t  y el modelo queda\n",
      "\n",
      "Y_t = S_t * (A + B * t) * E_t \n",
      "\n",
      "\tPor otro lado si la estacionalidad se presenta en forma de porcentaje se realiza la siguiente transformación exponencial donde  T_t = P ∗ (1 + i) ^t, con P constante e índice de crecimiento i. Así, el modelo queda:\n",
      "\n",
      "Y_t = S_t ∗ (P ∗ (1 + i)^ t ) ∗ E_t \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MODELO DE BROWN O DE SUAVIZACIÓN EXPONENCIAL DOBLE:\n",
      "\n",
      "\tLos modelos como de medias móviles o de suavización exponencial simple no permiten pronosticar la tendencia con anterioridad de manera adecuada presente en la demanda, esto se intenta corregir de mejor manera en el modelo de Holt.\n",
      "\n",
      "\tEste modelo es una modificación a la suavización exponencial simple, que introduce una constante de suavización, que tiene por objeto minimizar el error entre la demanda real y el pronóstico. Primero se realiza un suavización exponencial simple para luego realizar una segunda suavización exponencial con la constante mencionada anteriormente, y además utilizando los resultados de la primera suavización. El modelo entonces, es el siguiente:\n",
      "\n",
      "\u001a\u0019P\u001d",
      "t\u001b=α\u0019Y\u001d",
      "t\u001b + (1-α)\u0019P\u001d",
      "t-1\u001b\u001e",
      "\n",
      "\n",
      "Ver la ecuacion en la pagina 16 del paper: \n",
      "http://renanquispellanos.com/recursos/aporte%20intelectual/tecnicas%20prediccion/12.unidad9.pdf\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "b)\n",
      "\n",
      "MAPE:\n",
      "Error Porcentual Absoluto Medio o MAPE mide el tamaño del error absoluto porcentualmente entre la demanda real y la demanda pronosticada por nuestro modelo.\n",
      "Su principal ventaja es su fácil interpretación, ya que se usa en términos porcentuales independientemente de lo que se este estudiando, lo que la hace mas fácil de comprender.\n",
      "Formula:\n",
      "*\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error Promedio: corresponde a una métrica de error simple. Se calcula como la media aritmética de los errores. \n",
      "\n",
      "\t\t\t\t\tFORMULA\n",
      "\n",
      "En palabras simples es una medida de cuánto se sobreestima (o subestima) la predicción en promedio.\u000b",
      "El problema de esto, es que a diferencia del MAD, los errores pueden ser tanto positivos como negativos y “contrarrestarse” entre sí. A pesar de lo anterior, puede usarse para generalizar si existe una tendencia a sobreestimar o subestimar con la predicción utilizada dependiendo del signo del estadístico.\u000b",
      "\n",
      "MASE:  Error absoluto escalado medio (MASE por sus siglas en inglés Mean Absolute Scaled Error)\n",
      "\n",
      "\t\t\t\t\tFORMULA\n",
      "\n",
      "Es una métrica más compleja que las anteriores pero que puede ser utilizada en una gama mas amplia de casos, ya que es independiente de las escalas de los datos, lo que no genera los problemas de los clásicos métodos de medición de error.\n",
      "Como criterio, si MASE>1 implica que la proyección actual es peor que una proyección Naive en términos de Error Absoluto Medio. De la misma forma, si MASE<1 se puede hablar de una proyección mejor.\n",
      "\t\t\t\t\n",
      "\t\t\t\n",
      "\t\t\n",
      "\n",
      "\u000b",
      "\n",
      "INTERPRETACION MASE http://stats.stackexchange.com/questions/124365/interpretation-of-mean-absolute-scaled-error-mase\n",
      "\n",
      "\u000b",
      "Como ejemplo, tomaremos datos de un ejercicio de internet y aplicaremos las 3 medidas de error estadístico en función de una regresión lineal, como se ven a continuación:\n",
      "\n",
      "\n",
      "*\n",
      "*\n",
      "\n",
      "\n",
      "\n",
      "*\n",
      "\n",
      "\n",
      "Podemos observar que el MAPE es de 14,6%, mientras más pequeño sea este, mejor es el modelo, pues indica que hay un menor porcentaje de error promedio. Para este caso es relativamente alto, ya que la regresión lineal no pareciera ser el mejor modelo a usar. Con la medida del error promedio podemos interpretar que en promedio el modelo está sobre estimando la demanda en 3.75, debido a su signo negativo.\n",
      "Finalmente, el factor de MASE es de 0.778, lo que indica que como en menor a 1 el modelo del ejemplo es mejor que el Naive.\n",
      "\u0010\u0012\u001c",
      "\n",
      "\u0011\n",
      "\u0010\u0012\u001c",
      "\n",
      "\u001c",
      "\n",
      "\u0011\n",
      "c) Para esta parte, primero se trabajó con suavización exponencial con tendencia, con el cual se formuló el modelo que luego serviría para hacer las predicciones. Como se ve en el archivo Excel, se calculó además el MAD, la señal de rastreo y como método extra se calculó también el MAPE para cada modelo. Para los tres casos y más claramente para el de la leche y la electricidad se apreciaba claramente que existía una tendencia a aumentar la demanda expresada en el valor de “promedio de variación”, sujeta a un ciclo que se repetía a lo largo de las semanas de las que se contaba con información, es por esto que para realizar la predicción calculamos que los ciclos se repiten cada doce semanas, además de la variación de los datos entre dos ciclos. El promedio de esta variación se usó para, en base al último ciclo, pronosticar la demanda de las semanas siguientes. \n",
      "\n",
      "\tAdicionalmente se trabajó con el método de pronóstico de demanda Drift, con este modelo se puede pronosticar de buena manera la demanda de un periodo T al siguiente, pero luego cuando no se tiene los datos del periodo anterior, como se puede evidenciar claramente en el archivo excel para cada uno de los casos, el método de predicción pierde su eficacia. Para el pronóstico de las semanas donde se tiene información de la demanda se evidencia tanto tendencia como estacionalidad pero luego para las 52 semanas adicionales el modelo sólo conserva de buena manera la tendencia.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "d) Para optimizar cada uno de los modelos se procedió de manera similar, con ayuda de la herramienta SOLVER de excel, se encontraron los parámetros alpha y beta que minimizan la suma total del MAD, lo que entregó los parámetros que hacían que el modelo se ajustara de mejor forma. También se hizo lo mismo para minimizar el MAPE y la suma de la señal de rastreo lo que entrego parámetros similares al encontrado primeramente. Para el modelo Drift lo que se debe hacer para optimizar y disminuir los errores es modificar el parámetro H, y al hacerlo se observa que al llevarlo a valores menores mejora levemente el MAD,  la señal de rastreo y el MAPE, en cambio al llevarlo a valores altos, los errores aumentan. Esto se justifica de acuerdo a que H corresponde al intervalo de tiempo del pronóstico, por lo que a menor intervalo de tiempo el modelo se ajusta de mejor manera. Para este caso en particular el periodo de muestra es de 1 semana por lo que H es 1.\n",
      "\n",
      "Comparando ambos modelos para cada uno de los casos podemos observar que el modelo de suavización exponencial con tendencia se ajusta de mejor manera a los datos que el modelo drift. Esto se ve en que el primero tiene un menor MAPE que el segundo, lo que significa que en promedio el porcentaje de error entre los datos y la predicción es menor. Finalmente se eligió el modelo de suavización exponencial debido a que su error es menor, además que el sobreajuste es menor que en el modelo drift, ya que este predice basándose mucho en el dato de la demanda del periodo anterior, en cambio el de suavización exponencial desglosa las diferentes cosas que afectan a la demanda haciendo posible el proyectarlas en el futuro como un ponderador aislado.\n",
      "\n",
      "Fuentes parte 2a:\n",
      "https://en.wikipedia.org/wiki/Forecasting (Modelo de Drift)\n",
      "http://www.ingenieriaindustrialonline.com/ (Suavizacion exponencial doble)\n",
      "http://renanquispellanos.com/recursos/aporte%20intelectual/tecnicas%20prediccion/12.unidad9.pdf (ECUACION DE SUAVIZACION EXPONENCIAL DOBLE)\n",
      "http://www.emis.de/journals/HOA/ADS/Volume8_2/105.pdf (Modelo de Indice estacional multiplicativo)\n",
      "http://people.duke.edu/~rnau/Slides_on_forecasting_with_inflation_seasonal_adjustment_and_Winters_model--Robert%20Nau.pdf (Modelo de Indice estacional multiplicativo)\n",
      "\u0003\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nlog = []\\nibis = 1\\nchange = ''\\nsi = 1\\nei = 1\\nfor i in range(len(changelog) - 1):\\n    if changelog[i][1] == changelog[i + 1][1] == 'insertion' and changelog[i][2] == changelog[i+1][2]-len(changelog[i][3]):\\n        change += changelog[i + 1][3]\\n    elif changelog[i][1] != changelog[i + 1][1] and changelog[i][1] == 'insertion':\\n        log.append((changelog[i][0], changelog[i][1], changelog[i][2], change+changelog[i][3]))\\n        si = changelog[i+1][2]\\n        ei = changelog[i+1][3]\\n    elif changelog[i][1] == changelog[i + 1][1] == 'deletion' and changelog[i][2]-changelog[i][3]+1 == changelog[i+1][2]:\\n        ei = changelog\\npprint(log)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "def get_changelog(path):\n",
    "    changelog = []\n",
    "\n",
    "    def parse_mlti(mts, timestamp):\n",
    "        for i in range(len(mts)):\n",
    "            if mts[i]['ty'] == 'is':\n",
    "                changelog.append((timestamp, 'insert', mts[i]['ibi'], mts[i]['s']))\n",
    "                # print(mts[i]['ibi'], mts[i]['s'])\n",
    "            elif mts[i]['ty'] == 'ds':\n",
    "                changelog.append((timestamp, 'delete', mts[i]['si'], mts[i]['ei']))\n",
    "                # print(mts[i]['si'], mts[i]['ei'])\n",
    "            elif mts[i]['ty'] == 'mlti':\n",
    "                parse_mlti(mts[i]['mts'], timestamp)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    with open(\"../data/\" + path, 'rt', encoding=\"utf8\") as f:\n",
    "        for l in islice(f, 1):\n",
    "            # print(l)\n",
    "            data = json.loads(l)\n",
    "            # print(json.dumps(data,indent=2, separators=(',', ': ')))\n",
    "            for i in range(len(data['changelog'])):  # timestamp\n",
    "                timestamp = data['changelog'][i][1]\n",
    "                # print('timestamp:', timestamp)\n",
    "                if data['changelog'][i][0]['ty'] == 'is':\n",
    "                    pos = data['changelog'][i][0]['ibi']\n",
    "                    s = data['changelog'][i][0]['s']\n",
    "                    # print(pos, s)\n",
    "                    changelog.append((timestamp, 'insert', pos, s))\n",
    "                if data['changelog'][i][0]['ty'] == 'ds':\n",
    "                    start = data['changelog'][i][0]['si']\n",
    "                    end = data['changelog'][i][0]['ei']\n",
    "                    # print(start, end)\n",
    "                    changelog.append((timestamp, 'delete', start, end))\n",
    "                if data['changelog'][i][0]['ty'] == 'mlti':\n",
    "                    parse_mlti(data['changelog'][i][0]['mts'], timestamp)\n",
    "    return changelog\n",
    "\n",
    "\n",
    "def save_changelog(name, changelog):\n",
    "    f = open('../changelogs/' + name, mode='w', encoding=\"utf8\")\n",
    "    for l in changelog:\n",
    "        if l[1] == 'deletion':\n",
    "            f.write(str(l[0]) + ',' + str(l[1]) + ',' + str(l[2]) + ',' + str(l[3]) + '\\n')\n",
    "        else:\n",
    "            f.write(str(l[0]) + ',' + str(l[1]) + ',' + str(l[2]) + ',' + repr(str(l[3])) + '\\n')\n",
    "    f.flush()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def graph(name, changelog):\n",
    "    cum = [0]\n",
    "    ins = [0]\n",
    "    dls = [0]\n",
    "    pos = [0]\n",
    "    for x in range(len(changelog)):\n",
    "        if changelog[x][1] == 'insert':\n",
    "            cum.append(cum[x] + len(changelog[x][3]))\n",
    "            ins.append(ins[x] + len(changelog[x][3]))\n",
    "            dls.append(dls[x])\n",
    "        if changelog[x][1] == 'delete':\n",
    "            cum.append(cum[x] - (changelog[x][3] - changelog[x][2] + 1))\n",
    "            ins.append(ins[x])\n",
    "            dls.append(changelog[x][3] - changelog[x][2] + 1)\n",
    "        pos.append(changelog[x][2])\n",
    "    # plt.plot(ins)\n",
    "    plt.plot(np.array(cum))\n",
    "    # plt.plot(np.array(cum)-np.array(ins))\n",
    "    plt.plot(pos)\n",
    "    # plt.plot(np.array(cum)-np.array(dls))\n",
    "    plt.savefig('../pos_graph/' + name + '.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def time_graph(name, changelog):\n",
    "    ts = [changelog[0][0]]\n",
    "    cum = [0]\n",
    "    ins = [0]\n",
    "    dls = [0]\n",
    "    pos = [0]\n",
    "    for x in range(len(changelog)):\n",
    "        if changelog[x][1] == 'insert':\n",
    "            cum.append(cum[x] + len(changelog[x][3]))\n",
    "            ins.append(ins[x] + len(changelog[x][3]))\n",
    "            dls.append(dls[x])\n",
    "        if changelog[x][1] == 'delete':\n",
    "            cum.append(cum[x] - (changelog[x][3] - changelog[x][2] + 1))\n",
    "            ins.append(ins[x])\n",
    "            dls.append(changelog[x][3] - changelog[x][2] + 1)\n",
    "        pos.append(changelog[x][2])\n",
    "        ts.append(changelog[x][0])\n",
    "    # plt.plot(ins)\n",
    "    plt.plot(ts, np.array(cum))\n",
    "    # plt.plot(np.array(cum)-np.array(ins))\n",
    "    plt.plot(ts, pos)\n",
    "    # plt.plot(np.array(cum)-np.array(dls))\n",
    "    plt.savefig('../time_graph/' + name + '.png', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def recover_text(changelog):\n",
    "    text = ''\n",
    "    for i in range(len(changelog)):\n",
    "        type = changelog[i][1]\n",
    "        if type == 'insert':\n",
    "            ibi = changelog[i][2]\n",
    "            text = text[:ibi - 1] + changelog[i][3] + text[ibi - 1:]\n",
    "        if type == 'delete':\n",
    "            text = text[:changelog[i][2] - 1] + text[changelog[i][3]:]\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_changelogs():\n",
    "    for file in listdir(\"../data\"):\n",
    "        print(file)\n",
    "        save_changelog(file[:2] + '.csv', get_changelog(file))\n",
    "\n",
    "cl = get_changelog('56.txt')\n",
    "t = recover_text(cl)\n",
    "print(t)\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "log = []\n",
    "ibis = 1\n",
    "change = ''\n",
    "si = 1\n",
    "ei = 1\n",
    "for i in range(len(changelog) - 1):\n",
    "    if changelog[i][1] == changelog[i + 1][1] == 'insertion' and changelog[i][2] == changelog[i+1][2]-len(changelog[i][3]):\n",
    "        change += changelog[i + 1][3]\n",
    "    elif changelog[i][1] != changelog[i + 1][1] and changelog[i][1] == 'insertion':\n",
    "        log.append((changelog[i][0], changelog[i][1], changelog[i][2], change+changelog[i][3]))\n",
    "        si = changelog[i+1][2]\n",
    "        ei = changelog[i+1][3]\n",
    "    elif changelog[i][1] == changelog[i + 1][1] == 'deletion' and changelog[i][2]-changelog[i][3]+1 == changelog[i+1][2]:\n",
    "        ei = changelog\n",
    "pprint(log)\n",
    "\"\"\"\"\"\"\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
